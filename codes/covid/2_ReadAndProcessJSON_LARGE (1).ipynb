{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining COVID-19 Kaggle competition scientific papers to build an understanding of viruses\n",
    "## Part 2. Processing and featurizing data\n",
    "\n",
    "Working off of a clean metadata file, in this notebook we will featurize the subset of the JSON files that we downloaded from AI2 S3 repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import dask_cudf\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import cupy\n",
    "\n",
    "from distributed import Client\n",
    "\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the processed files\n",
    "All the data is located in our S3 bucket: `s3://bsql/data/covid`. However, since the process is quite lenghty we already uploaded the cleaned up version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 's3://bsql/data/covid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://54.209.32.248:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='http://54.209.32.248:8787/status' target='_blank'>http://54.209.32.248:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>16</li>\n",
       "  <li><b>Memory: </b>65.93 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://172.31.17.155:8786' processes=4 threads=16, memory=65.93 GB>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client('54.209.32.248:8786')\n",
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = dask_cudf.read_parquet(f'{data_dir}/full_text_clean.parquet').persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many paragraphs we have in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2,850,235 paragraps in the articles DataFrame\n",
      "The DataFrame consumes 2.3GB\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(articles):,} paragraps in the articles DataFrame')\n",
    "print(f'The DataFrame consumes {(articles.memory_usage().sum().compute()) / 1024**3:,.2}GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data featurization\n",
    "\n",
    "First step on the way to featurize our dataset - we need to create a vocabulary file. The vocabulary needs to conform to the same format as it is expected by the BERT models. \n",
    "\n",
    "## Build vocabulary\n",
    "\n",
    "In the first step we will simply tokenize the strings into words, normalize the strings to lower, and remove some of the punctuation signs we don't need. The `tokenize()` method splits a string on a space and puts every tokenized word in a `cudf.Series`. Next, we aggregate and count the occurence of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 31093\n"
     ]
    }
   ],
   "source": [
    "def tokenize_frame(frame, col):\n",
    "    temp = frame[col].str.tokenize().to_frame()\n",
    "    temp['text'] = temp['text'].str.lower()\n",
    "    temp['text'] = temp['text'].str.replace('[\\.?,#\"$!;:=\\(\\)\\-\\+0-9]', '')\n",
    "    temp['counter'] = 1\n",
    "    return temp\n",
    "\n",
    "min_count = 300\n",
    "\n",
    "token_counts = (\n",
    "    articles\n",
    "    .map_partitions(tokenize_frame, 'text')\n",
    "    .groupby('text')\n",
    "    .count()\n",
    "    .reset_index()\n",
    "    .sort_values(by='counter', ascending=False)\n",
    "    .query(f'counter > {min_count}')\n",
    ")\n",
    "\n",
    "token_counts = token_counts.compute().to_pandas()\n",
    "\n",
    "print(f'Total number of tokens: {len(token_counts)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look what this looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>counter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102067</th>\n",
       "      <td>dvgs</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224936</th>\n",
       "      <td>perth</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315619</th>\n",
       "      <td>complacency</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538209</th>\n",
       "      <td>rpi</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595962</th>\n",
       "      <td>skp</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               text  counter\n",
       "102067         dvgs      301\n",
       "224936        perth      301\n",
       "315619  complacency      301\n",
       "538209          rpi      301\n",
       "595962          skp      301"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the final vocabulary we will be using a `SubwordTextEncoder` from this repository: https://github.com/kwonmha/bert-vocab-builder/. The script we use is further slightly modified to remove the dependency on Tensorflow.\n",
    "\n",
    "The algorithm scans the words and iteratively builds a vocabulary of the longest subwords that the original words can be subdivided into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import text_encoder\n",
    "\n",
    "sw = text_encoder.SubwordTextEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SubwordTextEncoder` expects a dictionary with keys being the words and the values being the word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total vocab size : 12989, 2.406862735748291 seconds elapsed \n"
     ]
    }
   ],
   "source": [
    "token_counts_dict = dict(token_counts.to_dict('split')['data'])\n",
    "\n",
    "sw.build_from_token_counts(\n",
    "      token_counts_dict\n",
    "      , 3000\n",
    "      , 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look what we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = (\n",
    "    cudf.Series(sw._all_subtoken_strings)\n",
    "    .sort_values()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "with open('vocabulary_LARGE.txt', 'w') as f:\n",
    "    f.writelines([f'{item}\\n' for item in list(vocab.to_array())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the hash version of the vocabulary\n",
    "The `subword_tokenizer` requires an encoded version of the vocabulary to tokenize to the representation BERT is expecting. The script from CLX achieves that: https://github.com/rapidsai/clx/blob/80d3198dfe54bef704d177404873d2312a77f2c9/python/clx/analytics/perfect_hash.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to build table using 5.467472n space\n",
      "Longest bin was 169\n",
      "Number of bins: 3247\n",
      "Processing bin 0 size 3\n",
      "Processing bin 1000 size 2\n",
      "Processing bin 2000 size 9\n",
      "Processing bin 3000 size 6\n",
      "Final table size 64523 elements compared to 12989 for original\n",
      "Max bin length was 169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/jupyter/webinars/covid/scripts/perfect_hash.py:46: RuntimeWarning: overflow encountered in ulong_scalars\n",
      "  return ((a*k + b) % PRIME) % size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All present tokens return correct value.\n"
     ]
    }
   ],
   "source": [
    "from scripts import perfect_hash\n",
    "\n",
    "perfect_hash.hash_vocab(\n",
    "    'vocabulary_LARGE.txt'\n",
    "    , 'vocabulary_hash_LARGE.txt'\n",
    "    , False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize text\n",
    "Now we are ready to tokenize the text. First, we need to push the `vocabulary_hash_LARGE.txt` to each worker so the `subword_tokenizer` can read it. Let's first check how big is our file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432K\tvocabulary_hash_LARGE.txt\n"
     ]
    }
   ],
   "source": [
    "!du -h vocabulary_hash_LARGE.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiny. Let's upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upload_file('vocabulary_hash_LARGE.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "\n",
    "def subword_tokenize_frame(frame):\n",
    "    directory = '/mnt/blazingsql/dask-worker-space/dask-worker-space'\n",
    "    list_dirs = os.listdir(directory)\n",
    "    \n",
    "    pat = re.compile('(worker-[a-z0-9_]+)(?!\\.dirlock)')\n",
    "    local_folder = list(filter(pat.fullmatch, list_dirs))[0]\n",
    "    vocab_file = f'{directory}/{local_folder}/vocabulary_hash_LARGE.txt'\n",
    "    \n",
    "    num_strings = len(frame.text)\n",
    "    num_bytes = frame.text.str.byte_count().sum()\n",
    "\n",
    "    tokens, attention = frame.text.str.subword_tokenize(\n",
    "        vocab_file\n",
    "        , 256\n",
    "        , 256\n",
    "        , max_num_strings=num_strings\n",
    "        , max_num_chars=num_bytes\n",
    "        , max_rows_tensor=num_strings\n",
    "        , do_lower=False\n",
    "        , do_truncate=True\n",
    "    )[:2]\n",
    "    \n",
    "    temp = cudf.DataFrame()\n",
    "    temp['tokens'] = tokens\n",
    "    temp['attention'] = attention\n",
    "    \n",
    "    return temp\n",
    "tokenized = articles.partitions[:].map_partitions(subword_tokenize_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many tokens we get from all the articles we read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 729,660,160 tokens in the dataset.\n",
      "CPU times: user 51.1 ms, sys: 7.24 ms, total: 58.4 ms\n",
      "Wall time: 804 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokens_cnt = len(tokenized)\n",
    "print(f'There are {tokens_cnt:,} tokens in the dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>attention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2338</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3573</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>383</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1941</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tokens  attention\n",
       "0    2338          1\n",
       "1    7127          1\n",
       "2    3573          1\n",
       "3     383          1\n",
       "4    1941          1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each token has a maximum (padded) length of 256, if we divide the total number of tokens by 256 we should get the total number of paragraphs in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of paragraphs derived from tokens: 2,850,235, actual number of paragraphs: 2,850,235\n"
     ]
    }
   ],
   "source": [
    "articles_cnt = len(articles)\n",
    "\n",
    "assert tokens_cnt / 256 == articles_cnt\n",
    "print(f'Number of paragraphs derived from tokens: {int(tokens_cnt / 256):,}, actual number of paragraphs: {articles_cnt:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about memory usage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame consumes 5.4GB\n"
     ]
    }
   ],
   "source": [
    "print(f'The DataFrame consumes {(tokenized.memory_usage().sum().compute()) / 1024**3:,.2}GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might feel small even for a T4 with 16GB. However, as it stands today, the `subwords_tokenizer` method requires 21x the number of bytes as are stored in the string columns. https://docs.rapids.ai/api/cudf/nightly/api.html#cudf.core.column.string.StringMethods.subword_tokenize\n",
    "\n",
    "If you run `.compute()` on this DataFrame it will explode very quickly and OOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
